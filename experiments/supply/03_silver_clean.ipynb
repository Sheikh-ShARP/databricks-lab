{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "061d61d9-bedb-4053-859f-3079ce32f671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze = spark.table(\"supply_dev.bronze.makeup_supply_chain_raw\")\n",
    "display(df_bronze.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d55790f6-0fad-42bc-8dc8-00233d3376c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Product & Sales Information\n",
    "| Field                        | Description                                     |\n",
    "| ---------------------------- | ----------------------------------------------- |\n",
    "| **Product Type**             | Category or type of product in the supply chain |\n",
    "| **SKU (Stock Keeping Unit)** | Unique identifier for each product              |\n",
    "| **Price**                    | Selling price of the product                    |\n",
    "| **Number of Products Sold**  | Quantity of units sold in a given period        |\n",
    "| **Revenue Generated**        | Revenue from product sales                      |\n",
    "\n",
    "## Customer Information\n",
    "| Field                     | Description                                            |\n",
    "| ------------------------- | ------------------------------------------------------ |\n",
    "| **Customer Demographics** | Customer characteristics (age, gender, location, etc.) |\n",
    "\n",
    "## Inventory & Stock\n",
    "| Field            | Description                 |\n",
    "| ---------------- | --------------------------- |\n",
    "| **Availability** | Product availability status |\n",
    "| **Stock Levels** | Quantity currently in stock |\n",
    "\n",
    "## Orders & Shipping\n",
    "| Field                    | Description                                 |\n",
    "| ------------------------ | ------------------------------------------- |\n",
    "| **Order Quantities**     | Number of units in each order               |\n",
    "| **Shipping Times**       | Time taken to deliver products              |\n",
    "| **Shipping Carriers**    | Carrier or service responsible for shipment |\n",
    "| **Shipping Costs**       | Cost associated with shipping               |\n",
    "| **Transportation Modes** | Mode of transport (air, sea, land)          |\n",
    "| **Routes**               | Shipping paths used for delivery            |\n",
    "\n",
    "## Suppliers & Manufacturing\n",
    "| Field                       | Description                                   |\n",
    "| --------------------------- | --------------------------------------------- |\n",
    "| **Supplier Name**           | Vendor providing the product/material         |\n",
    "| **Location**                | Warehouse, supplier, or distribution location |\n",
    "| **Lead Time**               | Time required to receive goods from supplier  |\n",
    "| **Production Volumes**      | Units produced in a given period              |\n",
    "| **Manufacturing Lead Time** | Time required to manufacture a product        |\n",
    "| **Manufacturing Costs**     | Costs associated with production              |\n",
    "\n",
    "## Quality & Inspection\n",
    "| Field                  | Description                               |\n",
    "| ---------------------- | ----------------------------------------- |\n",
    "| **Inspection Results** | Outcome of quality checks                 |\n",
    "| **Defect Rates**       | Percentage or count of defective products |\n",
    "\n",
    "## General Cost Information\n",
    "| Field     | Description                                      |\n",
    "| --------- | ------------------------------------------------ |\n",
    "| **Costs** | Operational costs across supply chain activities |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce3077af-86f1-4e75-97f8-f2064b6faa95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(df_bronze.count(), len(df_bronze.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "051f3f07-609c-476c-9121-ccd7b189ac0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_bronze.select(\"Product_type\").distinct())\n",
    "display(df_bronze.select(\"Customer_demographics\").distinct())\n",
    "display(df_bronze.select(\"Shipping_carriers\").distinct())\n",
    "display(df_bronze.select(\"Transportation_modes\").distinct())\n",
    "display(df_bronze.select(\"Location\").distinct())\n",
    "display(df_bronze.select(\"Supplier_name\").distinct())\n",
    "display(df_bronze.select(\"Inspection_results\").distinct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc28f216-e7af-45fb-a7d8-10e2c0ebcfe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Count missing (null) values for each column in the DataFrame\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "missing_counts = (\n",
    "    df_bronze\n",
    "    .select([spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_bronze.columns])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14f2be03-70cf-4c4a-b245-ac238dcc1733",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(missing_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22fef21f-beac-45e0-8a20-3c56964422b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, trim, length\n",
    "\n",
    "# Count the number of empty (after trimming) string values in each column\n",
    "empty_string_counts = df_bronze.select([\n",
    "    spark_sum((trim(col(c)) == \"\").cast(\"int\")).alias(c)\n",
    "    for c in df_bronze.columns\n",
    "])\n",
    "\n",
    "display(empty_string_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c25c3d98-8b74-4ee0-9978-27522ef15d06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count the number of values that are only whitespace or empty (after trimming) in each column\n",
    "whitespace_counts = df_bronze.select([\n",
    "    spark_sum((length(trim(col(c))) == 0).cast(\"int\")).alias(c)\n",
    "    for c in df_bronze.columns\n",
    "])\n",
    "\n",
    "display(whitespace_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e9a9c86-e1a9-4a41-858c-d836f7efac1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count the number of rows where Price is zero and Order_Quantities is less than or equal to zero\n",
    "df_bronze.select([\n",
    "    spark_sum((col(\"Price\") == 0).cast(\"int\")).alias(\"price_zero\"),\n",
    "    spark_sum((col(\"Order_Quantities\") <= 0).cast(\"int\")).alias(\"bad_quantity\")\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b30a552-59e8-4f30-b4ab-b2a0cfe1b15b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Count the number of distinct values in each column of the DataFrame\n",
    "df_bronze.select([\n",
    "    countDistinct(col(c)).alias(c) for c in df_bronze.columns\n",
    "]).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a916819-1a76-4015-a83d-3395977bf198",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import (\n",
    "    col, trim, length, sum as spark_sum, countDistinct, lit\n",
    ")\n",
    "\n",
    "def run_data_quality(df: DataFrame, table_name: str, numeric_rules: dict = None) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Runs generic data quality checks on a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Input Spark DataFrame\n",
    "    - table_name: Logical table name for reporting\n",
    "    - numeric_rules: Optional dict like {\"price\": \"> 0\", \"quantity\": \"> 0\"}\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame containing DQ results\n",
    "    \"\"\"\n",
    "\n",
    "    total_rows = df.count()\n",
    "    results = []\n",
    "\n",
    "    # -------- Column-level checks --------\n",
    "    for c in df.columns:\n",
    "        stats = df.select(\n",
    "            spark_sum(col(c).isNull().cast(\"int\")).alias(\"nulls\"),\n",
    "            spark_sum((length(trim(col(c))) == 0).cast(\"int\")).alias(\"empty_strings\"),\n",
    "            countDistinct(col(c)).alias(\"distinct_count\")\n",
    "        ).collect()[0]\n",
    "\n",
    "        results.append((table_name, c, \"null_values\", stats[\"nulls\"], total_rows))\n",
    "        results.append((table_name, c, \"empty_or_whitespace\", stats[\"empty_strings\"], total_rows))\n",
    "        results.append((table_name, c, \"distinct_values\", stats[\"distinct_count\"], total_rows))\n",
    "\n",
    "    # -------- Numeric sanity checks (config-driven) --------\n",
    "    if numeric_rules:\n",
    "        for col_name, rule in numeric_rules.items():\n",
    "            if col_name in df.columns:\n",
    "                condition = ~eval(f\"col('{col_name}') {rule}\")\n",
    "                issue_count = df.filter(condition).count()\n",
    "                results.append((table_name, col_name, f\"rule_violation ({rule})\", issue_count, total_rows))\n",
    "\n",
    "    # -------- Create result DataFrame --------\n",
    "    dq_df = spark.createDataFrame(\n",
    "        results,\n",
    "        [\"table_name\", \"column_name\", \"check_type\", \"issue_count\", \"total_rows\"]\n",
    "    )\n",
    "\n",
    "    dq_df = dq_df.withColumn(\n",
    "        \"status\",\n",
    "        col(\"issue_count\") == 0\n",
    "    )\n",
    "\n",
    "    return dq_df\n",
    "\n",
    "\n",
    "# numeric_rules = {\n",
    "#     \"price\": \"> 0\",\n",
    "#     \"quantity\": \"> 0\",\n",
    "#     \"stock_levels\": \">= 0\"\n",
    "# }\n",
    "\n",
    "# dq_report = run_data_quality(df_bronze, \"orders_raw\", numeric_rules)\n",
    "# display(dq_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1c43c71-3c16-4482-a28a-25c9d34207b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"supply_dev.silver.orders\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "079f4e78-a11f-4173-8e2a-e18275a4e623",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_silver_clean",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
